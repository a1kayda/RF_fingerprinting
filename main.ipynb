{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/a1kayda/RF_fingerprinting/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jsvjpDg6LHux"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import DBSCAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sdNxkyP1MfzE"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tq_Fa2jxLRwJ",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "colnames = [\"S_Mean\", \"S_Max-Min\", \"S_Max-|Min|\", \"S_sumYup - sumYdn\", \"S_(Max-Mean)/(Mean-Min))\", \"S_numYup - numYdn\", \n",
        "            \"S_maxBd\", \"S_RangeJDy\", \"S_F\", \n",
        "            \"L_Mean\", \"L_Max-Min\", \"L_Max-|Min|\", \"L_sumYup - sumYdn\", \"L_(Max-Mean)/(Mean-Min))\", \"L_numYup - numYdn\", \n",
        "            \"L_maxBd\", \"L_RangeJDy\", \"L_F\", \"L_Ph\",\n",
        "            \"y\"]\n",
        "\n",
        "phase_colnames = []\n",
        "abs_colnames = []\n",
        "imag_colnames = []\n",
        "real_colnames = []\n",
        "for f_name in colnames:\n",
        "    phase_colnames.append(f_name + \"_phase\")\n",
        "    abs_colnames.append(f_name + \"_abs\")\n",
        "    real_colnames.append(f_name + \"_real\")\n",
        "    imag_colnames.append(f_name + \"_imag\")\n",
        "\n",
        "data_abs = pd.read_csv('/content/drive/MyDrive/Alkayda/Multipath_dataset_ABS_no_CFO_comp.txt', sep=\",\", names=abs_colnames, header=None)\n",
        "data_angle = pd.read_csv('/content/drive/MyDrive/Alkayda/Multipath_dataset_PHASE_no_CFO_comp.txt', sep=\",\", names=phase_colnames, header=None)\n",
        "\n",
        "data_abs.reset_index(drop=True, inplace=True)\n",
        "data_angle.reset_index(drop=True, inplace=True)\n",
        "\n",
        "data = pd.concat([data_abs.drop(\"y_abs\", axis=1), data_angle], axis=1)\n",
        "\n",
        "\n",
        "data = data.replace([np.inf, -np.inf, np.nan], 0)\n",
        "\n",
        "#data.reset_index()\n",
        "\n",
        "#data.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gU-ei32YLWsl"
      },
      "outputs": [],
      "source": [
        "X = np.array(data.drop(\"y_phase\", axis=1))\n",
        "y = np.array(data [\"y_phase\"]) - 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ejoIHckyLUs5"
      },
      "outputs": [],
      "source": [
        "# X_pca = PCA(n_components=2).fit_transform(X)\n",
        "# plt.scatter(X_pca[:, 0], X_pca[:, 1], c = y, cmap='autumn')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZhjXFWDCLYGF"
      },
      "outputs": [],
      "source": [
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "scaler.fit(X)\n",
        "X = scaler.transform(X)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y,random_state = 55,\n",
        "                                                        test_size = 0.3, shuffle=True, stratify=y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X4eCLkmzLaJC"
      },
      "outputs": [],
      "source": [
        "DT = DecisionTreeClassifier(random_state=60)\n",
        "DT.fit(X_train, y_train)\n",
        "\n",
        "y_pred = DT.predict(X_test)\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "print(acc)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f1 = []\n",
        "y_test_1 = y_test+1\n",
        "y_pred = y_pred+1\n",
        "for i in np.arange(1,11):\n",
        "  print(f1_score((y_test_1*(y_test_1==i)/i), (y_pred*(y_pred==i))/i))\n"
      ],
      "metadata": {
        "id": "iGooM7KeofUH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.unique(y_test+1)"
      ],
      "metadata": {
        "id": "VqZoLUfVs_0o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S7NA-mTLLUs_"
      },
      "outputs": [],
      "source": [
        "rf = RandomForestClassifier(n_estimators=300, criterion= \"entropy\", random_state=29)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = rf.predict(X_test)\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "print(acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aZAk1wt-LUtE"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(23,8))\n",
        "plt.bar(data.columns[:-1], rf.feature_importances_)\n",
        "plt.xticks(rotation=50)\n",
        "plt.show"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qdmXWb3pLUtG"
      },
      "outputs": [],
      "source": [
        "th = np.arange(0.025,0.06,0.005)\n",
        "features = [data.columns[np.append(rf.feature_importances_ > i, False)] for i in th]\n",
        "n_features = [np.size(i) for i in features]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "features[int(np.where(abs(th - 0.03) < 0.00001)[0])]"
      ],
      "metadata": {
        "id": "lGgMfYISwPaa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.where(abs(th - 0.03) < 0.00001)[0]"
      ],
      "metadata": {
        "id": "zdT-HRkswuvQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(20,10))\n",
        "plt.plot(th, n_features)\n",
        "plt.grid(True)\n",
        "plt.show"
      ],
      "metadata": {
        "id": "ebAEbow5qdWy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9me4b_tjLUtH"
      },
      "outputs": [],
      "source": [
        "rf_1 = RandomForestClassifier(n_estimators=100, criterion= \"entropy\")\n",
        "acc = []\n",
        "for i in th:\n",
        "      X_i = data[features[int(np.where(abs(th - i) < 0.00001)[0])]]\n",
        "      scaler.fit(X_i)\n",
        "      X_train, X_test, y_train, y_test = train_test_split(X_i, y,random_state = 55, test_size = 0.3, shuffle=True, stratify=y)\n",
        "      rf_1.fit(X_train, y_train)\n",
        "      y_pred = rf_1.predict(X_test)\n",
        "      acc.append(accuracy_score(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jbdJi8G3U718"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(20,10))\n",
        "plt.plot(n_features, acc)\n",
        "plt.grid(True)\n",
        "plt.show"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ncol = data.columns[np.append(rf.feature_importances_ > 0.03, False)]"
      ],
      "metadata": {
        "id": "mif98FsAl8Km"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CySbUdxDMTD-"
      },
      "outputs": [],
      "source": [
        "X = data[data.columns[np.append(rf.feature_importances_ > 0.03, False)]]\n",
        "scaler.fit(X)\n",
        "X = scaler.transform(X)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y,random_state = 55,\n",
        "                                                        test_size = 0.3, shuffle=True, stratify=y)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rf = RandomForestClassifier(n_estimators=300, criterion= \"entropy\", random_state=29)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = rf.predict(X_test)\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "print(acc)"
      ],
      "metadata": {
        "id": "I_dPJXx9m46A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nimportances = rf.feature_importances_\n",
        "a = list(ncol, nimportances)\n",
        "np.sort(a)\n",
        "\n",
        "plt.figure(figsize=(23,8))\n",
        "plt.bar(a[0], a[1])\n",
        "plt.xticks(rotation=50)\n",
        "plt.show"
      ],
      "metadata": {
        "id": "TQMqOYl_nClH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rf.feat"
      ],
      "metadata": {
        "id": "nPM9yIhumuE4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f_xDKU3iLUtJ"
      },
      "outputs": [],
      "source": [
        "from sklearn.manifold import TSNE"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_embedded = TSNE(n_components=2, learning_rate='auto', init='random').fit_transform(X)\n",
        "plt.plot(X_embedded)"
      ],
      "metadata": {
        "id": "d2X5L6n1ihjE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(y[y<2])"
      ],
      "metadata": {
        "id": "0sntsuMjoIbe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(X_embedded[y<6, 0], X_embedded[y<6, 1], c=y[y<6])"
      ],
      "metadata": {
        "id": "YqOmjZVAn6wV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_embedded[y<2].shape"
      ],
      "metadata": {
        "id": "MW8g_30to248"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_2=X_embedded[y<2]\n",
        "y_2 = y[y<2]"
      ],
      "metadata": {
        "id": "aU3R3K-blQfJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(X_2[:,0], X_2[:,1], c=y_2)"
      ],
      "metadata": {
        "id": "l8v__NBtkMdx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V9anOb8_LUtK"
      },
      "outputs": [],
      "source": [
        "clustering = DBSCAN(eps=0.223, n_jobs=-1)\n",
        "prediction = clustering.fit_predict(X_cutted)\n",
        "pred = prediction[np.argwhere(y == 1)]\n",
        "print(np.unique(clustering.labels_))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_embedded"
      ],
      "metadata": {
        "id": "CxOPw0MfkWe7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dMaFTCrnLUtK"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hnqVH0fXLUtL"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b8z5CdQ5LUtL"
      },
      "outputs": [],
      "source": [
        "# from sklearn.model_selection import cross_validate, StratifiedShuffleSplit\n",
        "\n",
        "# cv = StratifiedShuffleSplit(n_splits=5, random_state=3)\n",
        "# score = cross_validate(rf, X_train, y_train, cv=cv, scoring='accuracy', return_train_score=True)\n",
        "# print('rf', 'mean test score :', round(np.mean(score['test_score']),3), '\\n', 'train scores:', score['train_score'],  '\\n', 'test scores:', score['test_score'], '\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pTWzKtmCLUtM"
      },
      "outputs": [],
      "source": [
        "# from sklearn.neural_network import MLPClassifier\n",
        "# MLP = MLPClassifier(activation = 'relu', max_iter = 300, solver='adam', alpha=3e-4,\n",
        "#                         hidden_layer_sizes=(37, 250), random_state=10)\n",
        "# MLP.fit(X_train,y_train)\n",
        "\n",
        "# y_pred = MLP.predict(X_train)\n",
        "# acc = accuracy_score(y_train, y_pred)\n",
        "# print(acc)\n",
        "\n",
        "# y_pred = MLP.predict(X_test)\n",
        "# acc = accuracy_score(y_test, y_pred)\n",
        "# print(acc)\n",
        "\n",
        "# from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torchsummary\n",
        "# device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# fmnist_dataset_train = torchvision.datasets.FashionMNIST(root_path, \n",
        "#                                                         train=True, \n",
        "#                                                         transform=train_transform,\n",
        "#                                                         target_transform=None,\n",
        "#                                                         download=download)\n",
        "# fmnist_dataset_test = torchvision.datasets.FashionMNIST(root_path, \n",
        "#                                                        train=False, \n",
        "#                                                        transform=test_transform,\n",
        "#                                                        target_transform=None,\n",
        "#                                                        download=download)\n",
        "# train_loader = torch.utils.data.DataLoader(fmnist_dataset_train, \n",
        "#                                            batch_size=128,\n",
        "#                                            shuffle=True,\n",
        "#                                            num_workers=2)\n",
        "# test_loader = torch.utils.data.DataLoader(fmnist_dataset_test,\n",
        "#                                           batch_size=256,\n",
        "#                                           shuffle=False,\n",
        "#                                           num_workers=2)\n",
        "\n",
        "# class TinyNeuralNetwork(nn.Module):\n",
        "#     def __init__(self, input_shape=28*28, num_classes=10, input_channels=1, hidden_layer_size=(4096, 2048)):\n",
        "#         super(self.__class__, self).__init__()\n",
        "#         self.model = nn.Sequential(\n",
        "#             nn.Flatten(), # This layer converts image into a vector to use Linear layers afterwards\n",
        "#             # Your network structure comes here\n",
        "#             nn.Linear(input_shape, hidden_layer_size[0]),\n",
        "#             nn.Dropout(),\n",
        "#             nn.ELU(),\n",
        "#             nn.Linear(hidden_layer_size[0], hidden_layer_size[1]),\n",
        "#             nn.Dropout(),\n",
        "#             nn.ELU(),\n",
        "#             nn.Linear(hidden_layer_size[1], num_classes),\n",
        "#             nn.Dropout(),\n",
        "#             nn.LogSoftmax(dim=1)\n",
        "#         )\n",
        "        \n",
        "#     def forward(self, inp):       \n",
        "#         out = self.model(inp)\n",
        "#         return out\n",
        "\n",
        "# torchsummary.summary(TinyNeuralNetwork().to(device), (28*28,))\n",
        "\n",
        "# model = TinyNeuralNetwork().to(device)\n",
        "# opt = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "# loss_func = nn.functional.nll_loss\n",
        "\n",
        "# num_epochs = 60\n",
        "# def error(loader, model):\n",
        "#     model.eval()\n",
        "#     correct = 0\n",
        "#     with torch.no_grad():\n",
        "#         for data, target in loader:\n",
        "#             data, target = data.to(device), target.to(device)\n",
        "#             output = model(data)\n",
        "#             pred = output.argmax(dim=1, keepdim=True)\n",
        "#             correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "#     return 1 - correct / len(loader.dataset)\n",
        "\n",
        "# epoch_train_accs, epoch_test_accs = [], []\n",
        "# for epoch in range(num_epochs):\n",
        "#     epoch_train_accs.append(error(train_loader, model))\n",
        "#     epoch_test_accs.append(error(test_loader, model))\n",
        "\n",
        "#     clear_output(True)\n",
        "#     plt.plot(np.arange(len(epoch_train_accs)), epoch_train_accs, label='train error')\n",
        "#     plt.plot(np.arange(len(epoch_test_accs)), epoch_test_accs, label='test error')\n",
        "#     plt.legend()\n",
        "#     plt.show()\n",
        "\n",
        "#     model.train()\n",
        "#     correct = 0\n",
        "#     for data, target in train_loader:\n",
        "#         opt.zero_grad()\n",
        "#         output = model(data.to(device))\n",
        "#         loss = loss_func(output.to(device), target.to(device))\n",
        "#         loss.backward()\n",
        "#         opt.step()\n",
        "\n",
        "# print(f'Train accuracy: {1 - error(train_loader, model)}')\n",
        "# print(f'Test accuracy: {1 - error(test_loader, model)}')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "RF_fingerprinting_start.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}